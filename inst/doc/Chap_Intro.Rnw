\chapter{Introduction}
%%======================
\label{Chap-Intro}

\begin{Prov}
  This document was produced using  \textbf{Renext \Sexpr{Renext.Version}}
  with \textbf{R~\Sexpr{R.Version()$major}.\Sexpr{R.Version()$minor}}. 
  Function calls may have changed in subsequent versions of the package.
  More information on the \textbf{Renext} project can found at the 
  URL \url{https://gforge.irsn.fr/gf/project/renext}.
\end{Prov}

\section*{Acknowledgments}
%%---------------------------
%%The \textbf{Renext} package has been specified and implemented by the 
%%french \textit{Institut de Radioprotection et de S\^uret\'e Nucl\'eaire} 
%%(IRSN).
 
We gratefully acknowledge the BEHRIG\footnote{IRSN \textit{Bureau
    d'Expertise Hydrog\'eologique, Risques d'inondation et
    g\'eotechnique}.} members for their major contribution to
designing, documenting and testing programs or datasets: Claire-Marie
Duluc, Lise Bardet, Laurent Guimier and Vincent Rebour. We also
gratefully acknowledge Yann Richet who encouraged this project from
its beginning and provided assistance and useful advice.

\section{Goals}
%%-------------
The \textbf{Renext} package has been specified and implemented by the
french \textit{Institut de Radioprotection et de S\^uret\'e
  Nucl\'eaire} (IRSN).  The main goal is to implement in the R
environment~\citep{RMANUAL} the statistical framework known within the
community of french-speaking hydrologists as \textit{M\'ethode du
  Renouvellement} and partly devoted to Extreme Values (EV) problems.  This
methodology appeared during the years 1980 and was well-accepted both
by practitioners and researchers. The lack of freely
available software may have limited its applicability, but this method is
still in use or referred to.  The book in french by \citet{MIQUELBOOK}
is a frequently cited reference, while~\cite{PARENTBOOK} give a more
recent presentation.  Although some connections exist with the theory
of Renewal Processes~\citep{COXRENEWAL}, it must be said that the
standard application of the "Renouvellement" relies on the much
simpler Homogeneous Poisson Process (HPP)~\citep{COXISHAM}, and is
then similar to the famous Peaks Over Threshold (POT) method
\citep{DAVSMITH}.

POT methods are widespread and are described e.g. in the book
of~\citet{COLES} or that of~\citet{EKM}. There are several nice
R~packages devoted to POT or extreme values:
\verb@extRemes@~\citep{PACKextRemes}, \verb@ismev@~\citep{PACKismev},
\verb@evd@~\citep{PACKevd}, \verb@POT@~\cite{PACKPOT}, \verb@evir@~\cite{PACKevir},
\verb@evdbayes@~\citep{PACKevdbayes} among others.  The package
\verb@nsRFA@~\citep{PACKnsRFA} also contains useful functions for
Extreme Values modelling.

\smallskip
Yet Another POT package?
\begin{list}{$\bullet$}{
    \setlength{\itemsep}{2pt} 
    \setlength{\topsep}{2pt} 
  } 
\item Contrary to most POT packages, the distribution of the excesses
  over the threshold is not in \pkg{Renext} restricted to be in the
  Generalised Pareto Distributions (GPD) family and can be chosen
  within half a dozen of classical distributions including Weibull or
  gamma. Though theory says that GPD will be adequate for large enough
  thresholds, this is not a counter indication to the use of other
  distributions.  Fitting e.g. Weibull or gamma excesses might seem
  preferable to some practitioners and give good results for
  reasonably large return levels. 
  
\item The package allows the use of \textit{historical data} as
  explained in section~\ref{SECHISTORY}. Such data can have
  considerable importance in practical contexts since fairly large
  periods can be concerned.
\end{list}

Unlike most R~packages, \textbf{Renext} was not designed to implement
innovative techniques arising from recent research in statistics but
rather well accepted ones, as used by practitioners. The present
document is not intended to be a manual of extreme values modelling
but a presentation of the implemented tools with a limited statistical
description of these.

The general framework for estimation is \textit{Maximum Likelihood}
(ML) and a black-box maximisation can be used with a quite arbitrary
distribution of excesses. For the sake of generality, the inference
mainly relies on the approximate \textit{delta method}.  
\index{delta method} 
The present version does not allow the use of covariables.
The package allows extrapolation to fairly large return periods
(centuries). Needless to say, such extrapolations must be handled with
great care.

\section{Context and assumptions}
%%=====================================
\subsection{Assumptions}
%%-----------------------
\label{ASSUMPTIONS}
The general context is the modelling of a \textit{marked point process}
$[T_i,\,X_i]$.  \index{marked point process} Events (e.g. floods)
occur at successive random times $T_i$ when a random variable
"level"~$X_i$ is observed (e.g. flow).  We assume that only
\textit{large} values of the level~$X$ are of interest. Thus even if
the data are recorded on a regular basis (e.g. daily) the data can be
soundly pruned to remove small or even moderately large values of~$X$.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{images/POT.pdf}
  \caption{\label{POT} Events and levels. The random variable.
    $W_i=T_i-T_{i-1}$ can be called interevent.}
\end{figure}

Under some general assumptions the times $T_i$ corresponding to
large enough levels $X_i$ should be well described by an
\textit{Homogeneous Poisson Process}.  Recall that for HPP events the
number $N$ of events on a time interval of length~$w$ has a Poisson
distribution with mean $\mu_N=\lambda \times w$. Moreover the numbers
of $T_i$ corresponding to disjoint intervals are independent.  The
parameter $\lambda>0$ is called the \textit{rate} 
\index{rate, Poisson process} and has the physical dimension of an inverse time: it will
generally be given in inverse years or events by year. Another
important property of the HPP is that the interevent random
variables~$W_i=T_i-T_{i-1}$ are independent with the same exponential
distribution with mean $1/\lambda$.  \index{interevent}


Unless explicitly stated otherwise, we will make the following
assumptions about the marked process.
\begin{enumerate}
\item Events $T_i$ occur according to a Homogeneous Poisson Process
  with rate $\lambda$.
\item Levels $X_i$ form a sequence of independent identically
  distributed random variables with continuous distribution
  function~$F_X(x)$, survival function $S_X(x)= 1-F_X(x)$ and density
  $f_X(x)$.
\item The levels sequence and events sequence are independent.
  \index{survival function}
\end{enumerate}
The distribution $F_X(x)$ will be chosen within a parametric family
and depends on a vector of parameters $\bs{\theta}_X$. This dependence
can be enlightened using the notation~$F_X(x;\,\bs{\theta}_X)$ when
needed, the same convention applying to the density and the survival
functions.  The survival function can often in POT be preferred to the
distribution function.  The parameters of the whole model consist in
$\lambda$ and a vector $\bs{\theta}_X$.


\subsection{Return period, return level}
%%---------------------------------------------
\label{RETPERLEV}
The \textit{return period} \index{return period!in POT} 
of a given level~$x$
is the mean time between two events $T_i$ with levels exceeding~$x$,
that is with $X_i > x$. Under the assumptions above, it is given by
\begin{equation}
  \label{eq:RETPER}
  T(x) = \frac{1}{\lambda \,S_X(x)}.
\end{equation}
Indeed the probability of $\{X_i>x\}$ is $S_X(x)$, and the events
with level exceeding~$x$ also form an HPP\footnote{The is due to the
  independence of the two sequences $X_i$ and $T_i$.}  (thinned HPP)
with rate $\lambda \,S_X(x) $.  
\index{thinning (Poisson Process)}%
The mean interevent is the inverse rate.

Conversely, for a given period $T>0$ the \textit{return level} $x(T)$
is the level value $x$ having the return period $T$. It is given by
\begin{equation}
  \label{eq:RETLEV}
  x(T) = q_X(p),\quad p := 1 - \frac{1}{\lambda T}
\end{equation}
\index{return level!in POT}%
\noindent
where $q_X$ is the quantile function. The period $T$ must be greater
than $1/\lambda$. The limit of $x(T)$ for large periods is the upper
end-point of the distribution, which can be finite in some cases.
\index{end-point} 

In practice, the interest is often focused only on
large return levels or periods.


\subsection{Peaks Over Threshold  (POT)}
%%-------------------------------------
\index{POT (Peaks Over Threshold)|(}

\subsubsection*{The POT framework}
%%---------------------------------------
In the POT approach, only the upper part of the distribution $F_X(x)$
is modelled. More precisely, the interest is on the part $X > u$ where
$u$ is a \textit{threshold}.  The steps are 

\index{threshold!in POT}

\begin{list}{$\bullet$}{
  \setlength{\itemsep}{0pt} 
  \setlength{\topsep}{2pt}
  }
\item Fix a suitable threshold $u$,
\item Consider only the observations with level $X_i$ greater than $u$
  i.e. with $X_i>u$,
\item Estimate the rate of the events $X_i>u$ and fit a distribution 
  for the \textit{excesses} $Y_i = X_i -u$.
  \index{excess}
\end{list}
The distribution of $X$ conditional on $X>u$ is deduced from that of
the excess~$Y$ by translation.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{images/POTu.pdf}
  \caption{\label{POTu} In POT, only the levels $X_i$ with $X_i>u$
    are modeled through the excesses $Y_i=X_i-u$. The lower part $x <u$
    of the distribution $F_X(x)$ remains unknown.}
\end{figure}

The threshold will often be chosen above the mode of~$X$, leading to a
decreasing density for the excess~$Y$ as suggested on
figure~\ref{POTu}. The distribution of $Y$ typically has two
parameters.

\subsubsection*{Generalised Pareto Distribution}
%%----------------------------------------
The POT approach usually retains a GPD for the excess $Y$ or
equivalently a GPD for the level $X$ conditional on the exceedance $X
> u$. This choice is supported for a large threshold~$u$ by the
Pickands-Balkema-de Haan theorem
\index{Pickands-Balkema-de~Haan~theorem} (see~\ref{GEVGPD}) or by the
related \textit{POT-stability} property of the GPD
(see~\ref{GPDPROP}).  \index{POT stability} The family of GPDs with a
given shape parameter $\xi$ can be said to be POT-stable: if for a
given threshold $u$ the distribution of $X$ conditional on $X >u$ is a
GPD with shape $\xi$, then for any another threshold $v>u$ the
distribution of $X$ conditional on $X>v$ is still a GPD with the same
shape~$\xi$.  By selecting a threshold $v > u$ in POT, the estimation
will use a smaller set of $X_i$ but the underlying distribution of $X$
conditional on exceedance is the same in the two cases.
\index{exceedance}

\index{threshold!choice}
The choice of the threshold is a well-known difficulty in
classical POT where only GPD excesses are used~\citep{DAVSMITH}. 
The situation is much more complex with non-GPD excesses, because POT
stability no longer holds.
%% The family of GPDs with a given
%% shape parameter $\xi$ can be said "POT stable".  \index{POT stability}
%% With another threshold $v>u$ the estimation will use a smaller set of
%% $X_i$ but the underlying distribution of $X$ conditional on $X > v$ is
%% the same in the two cases.  
For instance if the excesses over
$u$ are Weibull with shape $\alpha>0$ and scale~$\beta = 1$ i.e.
$$
   \Pr\left\{\Cond{X>x}{X>u} \right\} =  \exp\left\{ - (x-u)^\alpha \right\} 
   \qquad x > u
$$
\index{Weibull distribution}%
%% then the conditional distribution over a higher threshold $v>u$ is
%% given by
%% $$
%%    \Pr\left\{\Cond{X>x}{X>v} \right\} = 
%% \exp\left\{ - \left[ (x-u)^\alpha -(v-u)^\alpha \right]\right\}   \qquad x > v > u
%% $$
then the conditional distribution of the excess $\Cond{X-v}{X>v}$ \textit{is not}
Weibull; it is a shifted version of the \index{left truncated Weibull}
\textit{Left Truncated Weibull} (LTW), see~\ref{SLTW}.

\index{POT (Peaks Over Threshold)|)}

\subsection{Link with Block Maxima}
%%---------------------------------------------
\index{block maxima|(} \index{rlargest@{$r$~largest}|(}%
\index{blocks}%

Alternative approaches to POT for univariate Extreme Values modelling
use time \textit{blocks} 
of, say, one year and related by-block data. Numerous
observations of the variable of interest $X$ are assumed to exist in
each block, and only the largest of them are retained in the
analysis. Popular examples are

\begin{list}{$\bullet$}{\setlength{\itemsep}{2pt}\setlength{\topsep}{2pt}}
  
\item \textbf{block maxima}: for each block, only the maximal value is
  used in the analysis.

\item \textbf{ $r$~largest}: for each block the largest~$r$
  observations (i.e. the $r$ largest order statistics) are recorded.
  The number~$r$ may vary across the blocks.  

\end{list}
Block maxima is the special case of $r$~largest for $r=1$, and using
$r>1$ largest observations when available leads to a better
estimation. The $r$~largest analysis is described in chap.~3 of the
book of \citet{COLES}. The distribution retained for the maxima or the
$r$~largest is based on asymptotic considerations. The block maxima
are usually assumed independent and to follow a Generalised Extreme
Value (GEV) distribution. From the Fisher-Tippett-Gnedenko theorem, 
this corresponds to the situation where $n$
independent and identically distributed $X_i$ are found in each block,
the number~$n$ being large --~see section~\ref{ASYMPTGEV}.



%% Underlying the block data, one would generally find a continuous time
%% process (e.g. temperature, sea surge), possibly observed at fixed
%% times (e.g. high tide). The time-length of the blocks is generally
%% chosen in order to reach a limit behaviour ignoring autocorrelation or
%% seasonality in the continuous process.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{images/RenouvAgc.pdf}
  \caption{\label{RenouvAg} Block maxima for the marked
    process. If the marks $X_i$ follow a GPD, then for a constant block duration
    the block maxima $M_b:=X_{i_b}$ follow a GEV distribution
    (provided that they exist).  
  }
\end{figure}

Interestingly, the assumptions concerning the marked point process as
stated before in~\ref{ASSUMPTIONS} provide a framework to derive the
distribution of the maxima or that of the $r$~largest observations
over non-overlapping blocks, without any asymptotic consideration.
Given $B$ such blocks $b=1$, $2$, $\dots$, $B$ with known duration~$w_b$, 
the maximum $M_b$ for block $b$ is the maximum of $N_b$ levels
$X_i$ with $T_i$ falling in that block, where $N_b$ has a Poisson
distribution with mean $\lambda w_b$.  Moreover the maxima $M_b$ are
independent across blocks. The distribution of the $M_b$ can be
related to the distribution of the marks: see appendix
page~\pageref{COMPOUND}.  Note however that the marked process can
lead to blocks $b$ with no observation, especially when the block
duration $w_b$ is not large relative to the mean interevent
$1/\lambda$.  Similarly, the joint distribution of the $r$~largest in
a block is easily derived, see section~\ref{SECHISTORY} later.

When the distribution of the marks $X_i$ is assumed to be GPD and the
blocks have the same duration $w_1$ (e.g. one year), the block maxima
$M_b$ are independent and follow a GEV distribution, as is usually
assumed for block maxima. It can be shown as well that the
distribution of the $r$~largest observations $X_i$ is then the distribution
used in the $r$~largest analysis Coles~\citep[chap.~3]{COLES},
provided that at least $r$ observations~$X_i$ exist. The notion of
\emph{return period} for the blocks framework differs from the one
given above see discussion~\ref{TWORL} page~\pageref{TWORL}.  However,
the difference between the two notions is confined to the small return
periods context.

To summarise:  maxima or $r$~largest observations can be viewed as
\textit{partial observations} \index{partial observation} of the
marked process, or as the result of a \textit{temporal aggregation}
\index{aggregation, temporal} of this process. When the result of such
an aggregation (i.e.  maxima or $r$~largest) is known for one or
several blocks with large duration, say decades or centuries, we may
speak of \textit{historical data}.  
\index{historical data}%

Although \textbf{Renext} primarily uses the original data
$[T_i,\,X_i]$ as described in~\ref{ASSUMPTIONS} above, it is possible
to make use of supplementary block data in a quite flexible
fashion. Maxima and $r$~largest observations within block(s) can also
be used, as well as the marks exceeding some known auxiliary threshold
as sometimes called a \textit{perception threshold}. A typical use of
these possibilities is for historical data.
\index{threshold!perception}%
\index{block maxima|)}
\index{rlargest@{$r$~largest}|)}%

\subsection{Declustering}
%%--------------------------------
\index{declustering}
Most of Extreme Values problems concern a continuous time process:
discharge flow, temperature, sea surge, etc.
POT modelling most often requires a \textit{declustering} step leading to
independent events: floods, heat or cold waves,
storms, etc. \pkg{Renext} does not currently provide any declustering
function, which can be found in the other POT packages cited above. 



\section{Heterogeneous data}
%%==============
\subsection{Remarks}
%%-----------------
Model fitting functions in~R usually have a formal argument specifying
data with a \textit{data frame} object, the model being typically
given by a \textit{formula}. Due to the presence of heterogeneous
types of data within a given ``dataset'', the arguments of
\textbf{Renext} functions will take a slightly more complex form. For
instance, it will generally be necessary to specify a duration or
several block durations in complement to the vector of levels, to specify
where missing periods (gaps) occurred, etc.

Some of the package functions require the use of \verb@POSIX@ objects
representing date and time. R~base package provides versatile
functions to manage date/time or timestamps. See for instance the help
of the \verb@strptime@ function.  
\index{POSIX objects@{\texttt{POSIX} objects}}

As most R~packages do, \textbf{Renext} comes with a few datasets taken
from relevant literature or from real data examples. These datasets
are given as lists objects with hopefully understandable element
names. Some datasets have an S3 class named \verb@"Rendata"@ and can
as such be used as the first formal argument of popular S3 methods: 
\verb@plot@, \verb@summary@ and more.

\subsection{OT data}
%%-----------------
\index{OTdata@{\texttt{OTdata}}|(}% 

The data used in POT will mainly consist in recorded levels $X_i$ or
levels exceeding a reasonably low known threshold~$u_\star$, with possibly
$u_\star=-\infty$. Such data will be called \textit{OT data} for
``Over Threshold''. The POT modelling will typically use a higher
suitably chosen threshold $u>u_\star$.

\index{Brest data@{\texttt{Brest} data}|(}
The data \verb@Brest@ contain sea surge heights at high
tide for the Brest gauging station. Only values exceeding $u_\star=30$\,cm
are retained. More details about these data are provided in the
package manual. The data are provided as a list with several parts.

<<>>=
library(Renext)
names(Brest)
@ 

\noindent
As their names may suggest the list elements contain Over Threshold
(OT) data and information.

<<>>=
head(Brest$OTdata, n = 4)
str(Brest$OTinfo)
@ 

\noindent
The \verb@OTdata@ element is a data frame giving the $T_i$ (in time
order) and the corresponding levels~$X_i$.  Note that the time part of
the \verb@POSIX@ object may not be relevant. Here only the date part
makes sense and the time part is by convention \verb@"00:00"@ with the
time zone set to \verb@"GMT"@ to use Coordinated Universal Time (UTC).
Of course, the observations were made at a different time.
\index{Coordinated Universal Time (UTC)}

%% However
%% on such a large period of time, it can be affected by \textit{leap seconds}, and
%% \verb@"00:00"@ might appear as \verb@"23:59"@ the day
%% before. 
%% \index{leap seconds}%

The \verb@OTinfo@ list mentions an \textit{effective duration}. This
is less than the time range which can be computed using the methods
\verb@range@ and \verb@diff@ from the \textbf{base} package
%% ## as.numeric(diff(range(Brest$OTdata$date), units= "days"))/365.25

<<<>>=
End <- Brest$OTinfo$end; Start <- Brest$OTinfo$start
Dur <- as.numeric(difftime(End,  Start, units = "days"))/365.25
Dur
Dur - as.numeric(Brest$OTinfo$effDuration) 
@

\noindent
The difference --~more than \Sexpr{floor(Dur-as.numeric(Brest$OTinfo$effDuration))} 
years~-- is due to gaps or \textit{missing periods}. 
\index{missing periods!description}%
The missing periods are described in the element \verb@OTmissing@.
%%The \verb@MAXdata@ and \verb@MAXinfo@ are here \verb@NULL@ but could 
%%contain data as in \verb@Garonne@ example described below.

The \verb@Brest@ dataset has class \verb@"Rendata"@. This is an S3 
class defined in \textbf{Renext} to describe objects containing
\verb@OTdata@ and possibly some extra information on missing periods
or historical data. It has a \verb@summary@ method 
\index{Rendata class@{\texttt{Rendata} class}|(}

<<label=summaryBrest>>=
class(Brest)
summary(Brest)
@ 

\noindent
The displayed information concerns the levels in the main OT sample
and the possible gaps in this sample: number, duration (in years).
A \verb@plot@ method also exists

<<label=RenplotBrest,  fig=TRUE, include=FALSE>>=
plot(Brest)
@ 

\noindent
which produces the plot on the left of figure~\ref{RENPLOTS}.

\index{Brest data@{\texttt{Brest} data}|)}
\index{OTdata@{\texttt{OTdata}}|)}%

\subsection{Missing periods or gaps}
%%--------------------------------
\index{missing periods!description}% 
\index{gaps|see{missing periods}}%
A common problem in POT modelling is the existence of gaps within the
observation period. These can result from many causes: damage or
failure of the measurement system, human errors, strikes, wars, ...

\textbf{Renext} uses a natural description of the gaps within a
dataset. They are stored as rows of a data.frame with two \verb@POSIX@
columns \verb@start@ and \verb@end@

<<label=missing>>=
head(Brest$OTmissing, n = 4)
@ 

\noindent
Missing periods must be taken into account in the analysis. They
should be displayed on timeplots showing events, since it is
important to make a distinction between periods with no events and
gaps, see figure~\ref{RENPLOTS}. An important prerequisite to modelling
is to ensure that the gaps occur independently from measured
variables. For instance, storms can damage gauging systems for wind or
sea level thus leading to \textit{endogenously missing} observations forming an 
endogenous gap. This may be considered as a form of censoring.

\index{missing periods!endogenous}
\index{censoring}

\subsection{Aggregated (block) data}
%%------------------------
\index{historical data}% 
\label{HistoricalData}
\index{block data|(}%

\subsubsection*{Motivation}
%%-------------------------
In a \verb@Rendata@ object, the ordinary data provided in the
\verb@OTdata@ element can be completed by some data observed in
blocks with known duration. This possibility is often
required to use historical information.  Two types of block data are
currently supported under the names "MAX" and "OTS" data.  These 
can be regarded as the two types of censored data: type~I
for OTS and type~II for MAX, and are described more precisely
in section~\ref{TwoTypes}
page~\pageref{TwoTypes}.

\subsubsection*{MAXdata}
%%----------------------------
\index{MAXdata@{\texttt{MAXdata}}}%
As a first possible complement to \verb@OTdata@, we may have \verb@MAXdata@ that
is: $r$~largest observations over one or several blocks. Such
data require a complementary information: the block duration(s) which
must be given in years.

\index{Garonne data@{\texttt{Garonne} data}|(}
The dataset \verb@Garonne@ is taken from~\citet{MIQUELBOOK} where
it is described.  The data concern the french river \textit{La
  Garonne} at the gauging station named \textit{Le Mas d'Agenais}
where many floods occurred during the past centuries.  The data
consist in both OT data and historical data. The variable is the river
discharge flow in $\textrm{m}^3/\textrm{s}$ as estimated from the river level
using a rating curve. The precision is limited and many ties are
present among the flow values.  The OT data contain flow values over
the threshold $u = 2500~\textrm{m}^3/\textrm{s}$. 

The historical data in \verb@Garonne@ are simply the~$12$ largest
flows for a period of about $143$~years and will be used later.

<<labelgaronneMAX>>=
names(Garonne)
Garonne$MAXinfo
head(Garonne$MAXdata, n = 4)
@ 

\noindent
The \verb@Garonne@ dataset has class \verb@"Rendata"@. The \verb@plot@
method for this class

<<label=RenplotGaronne, include=FALSE, fig=TRUE>>=
plot(Garonne)
@ 

\noindent
produces a graphic displaying the historical period as on the right
panel of figure~\ref{RENPLOTS}. Here the dates of the historical
events are not known exactly and thus are provided here as \verb@NA@
\verb@POSIXct@ objects. The historical levels are thus displayed as
horizontal segments, while vertical segments would be used for known
dates.  The \verb@plot@ method for the class \verb@Rendata@ has a
\verb@showHist@ logical formal argument telling that historical
periods should be shown (default value \verb@TRUE@) or not.


Note that the function \texttt{OT2MAX} can be used to compute the
$r$~largest values in blocks of one year from observations
$[T_i,\,X_i]$ of a marked process. This function can be used to
compare a POT approach to block maxima or $r$~largest,
see~\ref{OT2MAX}.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-RenplotBrest.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-RenplotGaronne.pdf} 
   \end{tabular}
   \caption{\label{RENPLOTS} Graphics produced using the \texttt{plot}
     method of the \texttt{"Rendata"} class. On the left, the
     \texttt{Brest} object contains missing periods that are shown.
     On the right, the \texttt{Garonne} dataset contains information
     about an \textit{historical period}, displayed as a green rectangle.  }
\end{figure}
\index{Garonne data@{\texttt{Garonne} data}|)}

It can be remarked here that working with the original unit leads to
observations with a quite large order of magnitude. This can be a
problem in some numerical evaluations such as the determination of a
hessian.  \index{hessian} Although the \verb@Renouv@ function used
later internally scales the data, it could be preferable to rescale
the data e.g. by dividing them by $1000$.
\index{rescaling (data)}
\index{scale of data|see{rescaling (data)}}

\subsubsection*{OTSdata}
%%----------------------------
\index{OTSdata@{\texttt{OTSdata}}}%

The other type of block data involves a number of $B$ non-overlapping
blocks.  For each block $b=1$, $2$, $\dots$, $B$ the duration $w_b$ is
assumed to be known as well as a threshold $u_b$. We then assume to be
given all the observations with level $X_i$ exceeding the threshold
$u_b$ i.e. with $X_i > u_b$.  It is assumed that no OTS threshold
$u_b$ is smaller than the OT threshold~$u$. In some cases the
times $T_i$ are known and can be provided in the \verb@date@ column
of the data frame \verb@OTSdata@.  Unlike with \verb@MAXdata@, one
block can be empty because no level $X_i$ exceeding $u_b$ was
found. The block will then appear in the \verb@OTSinfo@ data frame
but not in~\verb@OTSdata@.

\subsection{Overview}

The general structure of a \verb@Rendata@ object is described in
table~\ref{RENDATATAB}.

\begin{table}
  \centering
  \begin{tabular}{l l l }
    \toprule
    \multicolumn{1}{c}{element} & 
    \multicolumn{1}{c}{class} & 
    \multicolumn{1}{c}{content} \\ \toprule
    \texttt{info} $(\star)$   & \texttt{list}     & 
    general information: variable name, units, ...
    \\
    \texttt{describe}       & \texttt{character}  & optional description\\  \hline
    \texttt{OTinfo} $(\star)$ & \texttt{list}     & 
    \texttt{start}, \texttt{end}, \texttt{duration} $w$, \texttt{threshold} $u$\\
    \texttt{OTdata} $(\star)$ & \texttt{data frame} & 
    \texttt{date} $T_i$, \textit{level} $X_i$ and \texttt{comment} \\
    \texttt{OTMissing}      & \texttt{data frame} & 
     \texttt{start}, \texttt{end}, \texttt{comment} \\ \hline 
    \texttt{MAXinfo}        & \texttt{data frame} & 
    \texttt{start}, \texttt{end}, \texttt{duration} $w_b$\\
    \texttt{MAXdata}        & \texttt{data frame} & 
    \texttt{block}, \texttt{date}, \textit{level}, \texttt{comment}\\ \hline
    \texttt{OTSinfo}        & \texttt{data frame} & 
    \texttt{start}, \texttt{end}, \texttt{duration} $w_b$, \texttt{threshold} $u_b$
    \\
    \texttt{OTSdata}        & \texttt{data frame} & 
    \texttt{block}, \texttt{date}, \textit{level}, \texttt{comment}\\ \toprule
  \end{tabular}
  \caption{\label{RENDATATAB}
    Structure of a \texttt{Rendata} object. The required elements are marked 
    with a star $(\star)$. The threshold in \texttt{OTinfo} can be set to \texttt{-Inf},
    thus allowing the computations of the excesses $X - u$ for any threshold $u$.}
    
\end{table}

The \verb@readXML@ function (still experimental) can be used to read
such heterogeneous data from an XML file and possibly linking to csv files. Some
examples are shipped with the package, see help with \verb@?readXML@.
\index{XML}
\index{readXML function@{\texttt{readXML} function}}
  
\subsection{Simulating heterogeneous data}
%%-------------------------------
\index{simulation} 
\index{rRendata function@{\texttt{rRendata} function}|(}
Heterogeneous data generated by a Monte-Carlo simulation are of great
help in POT-based analysis. For instance, simulated data can be used
to assess the bias of an estimate, or to compare several plotting
positions. It also helps in getting familiar with the random
variations in the estimates or in the return level plots.  The
\verb@rRendata@ function can be used to generate a \verb@RenData@
object with a specific design: duration of the main sample, number and
durations of MAX data or OTS blocks.

Suppose that we use a main sample of (default) duration $100$ years
and the default distribution the standard exponential. We can enhance
the data by adding three MAX blocks of say $40$, $50$ and $30$
years. By default, only the maximum observation will be kept in each
block.

<<rRendata1, fig=TRUE, include=FALSE>>=
set.seed(1234)
RD1 <- rRendata(MAX.effDuration = c(40, 50, 30))
plot(RD1)
@

\noindent
See left of figure~\ref{RRENDATA}. The three MAX blocks are by
convention located before the start of the main sample since in
practice such blocks often represent historical data. We can similarly
add $3$ OTS blocks with $3$ choosen durations and thresholds.

<<rRendata2, fig=TRUE, include=FALSE>>=
RD2 <- rRendata(effDuration = 30,
                distname.y = "GPD",
                par.y = c(scale = 1, shape = 0.1),
                OTS.effDuration = c(40, 50, 30), OTS.threshold = c(3, 4, 2))
plot(RD2)
@ 

\noindent
Note that we used here a non-default \texttt{"GPD"} distribution for
the excesses $Y_i$, and we gave the values of the parameters. For now,
the \verb@rRendata@ function can not generate random missing periods.

\begin{figure}
   \centering
   \begin{tabular}{c c} 
     \includegraphics[width=7cm]{Rgraphics/fig-rRendata1.pdf} &
     \includegraphics[width=7cm]{Rgraphics/fig-rRendata2.pdf} 
   \end{tabular}
   \caption{\label{RRENDATA} 
     Two randomly generated \texttt{Rendata}
     objects. The distribution of the marks is exponential on the
     left, and GPD on the right. Three MAX blocks are used on the 
     left, and three OTS blocks are used on the right.
   }
\end{figure}
\index{rRendata function@{\texttt{rRendata} function}|)}
\index{Rendata class@{\texttt{Rendata} class}|)}
\index{block data|)}%

\subsection{Aggregated  data and gaps} 
%%----------------------------------
A difficulty with aggregated data such as block data concerns the
treatment of missing data or gaps. There is usually no reason that
missing periods should correspond to full blocks (e.g. years), and
most often a fraction of some blocks is missing. Excluding all blocks
with missing data leads to a severe loss of information, while
ignoring gaps in blocks may cause a bias.  The use of aggregated data
will be illustrated later in the section~\ref{CountData}
about \verb@barplotRenouv@. The problem of gaps in blocks will be also be
discussed when describing the \verb@OT2MAX@ function in
section~\ref{OT2MAX} p.~\pageref{OT2MAX}.
  
